{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BCS Secy Recruitment_Ritaban.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **In this simple neural network we will train our model on the familiar MNIST Database to recognize handwritten digits.**"
      ],
      "metadata": {
        "id": "MkvLmnv9pa3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the MNIST Database\n",
        "We import labeled handwritten digits from the MNIST database. This is a convenient repository as the images have been size-normalized into a 28x28 pixel bounding box. We further normalize the tensor images by means of the 'Normalize' transform using the mean (0.1307) and standard deviation of the dataset (0.3081). The output tensor is computed as, output = (x-mean)/std.\n",
        "\n"
      ],
      "metadata": {
        "id": "G-XwAZuTeRr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,))\n",
        "            ])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True, transform=transform))\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transform))"
      ],
      "metadata": {
        "id": "TmmVkivoEPWa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network with 4 Layers\n",
        "Here, I have employed the hyperbolic tangent function for the hidden layers, and the sigmoid function for the output layer. I specifically used the tanh function as it brings the mean close to zero, making the training easier for subsequent layers."
      ],
      "metadata": {
        "id": "wFsSx4dAhke6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, epochs=10):\n",
        "        super(Net, self).__init__()\n",
        "        self.linear1 = nn.Linear(784, 128)\n",
        "        self.linear2 = nn.Linear(128, 64)\n",
        "        self.linear3 = nn.Linear(64, 10)\n",
        "\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.linear2(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.linear3(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "    \n",
        "    def one_hot_encode(self, y):\n",
        "        encoded = torch.zeros([10], dtype=torch.float64)\n",
        "        encoded[y[0]] = 1.\n",
        "        return encoded\n",
        "\n",
        "    def train(self, train_loader, optimizer, criterion):\n",
        "        start_time = time.time()\n",
        "        loss = None\n",
        "\n",
        "        for iteration in range(self.epochs):\n",
        "            for x,y in train_loader:\n",
        "                y = self.one_hot_encode(y)\n",
        "                optimizer.zero_grad()\n",
        "                output = self.forward_pass(torch.flatten(x))\n",
        "                loss = criterion(output, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            print('Epoch: {0}, Time Spent: {1:.2f}s, Loss: {2}'.format(\n",
        "                iteration+1, time.time() - start_time, loss\n",
        "            ))"
      ],
      "metadata": {
        "id": "OjGlanDREFpU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "I have used the Binary Cross Entropy with Logit Loss as my loss fucntion as it was giving me better results as opposed to a simple Binary Cross Entropy. For the optimization function, I went with the reliable Adam optimizer, as it is relatively fast without compromising on accuracy."
      ],
      "metadata": {
        "id": "Dtc8onWTZT_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model.train(train_loader, optimizer, criterion)"
      ],
      "metadata": {
        "id": "5c8dZVIcDsoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad1f5b1-1660-44b0-bb19-f40be50f99a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Time Spent: 133.30s, Loss: 0.6931471825587292\n",
            "Epoch: 2, Time Spent: 327.62s, Loss: 0.693147182500419\n",
            "Epoch: 3, Time Spent: 549.89s, Loss: 0.6931471824867155\n"
          ]
        }
      ]
    }
  ]
}